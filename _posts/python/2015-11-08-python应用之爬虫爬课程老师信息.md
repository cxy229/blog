---
layout     : post
title      : python应用之爬大工教务
categories : python
tags       : [notes]
published  : false
---
#### crawlCourses.py
```python
#!/usr/bin/env python3
# -*- coding:utf-8 -*-

import requests
from bs4 import BeautifulSoup
import mysql.connector

#登陆mysql
#conn = mysql.connector.connect(host='主机地址',user='用户名', password='密码', database='dlut')
cursor = conn.cursor()

session = requests.Session()
#登陆
params = {'zjh':'学号','mm': '密码',}
r = session.post('http://202.118.65.21:8089/loginAction.do',params)
#概览界面
r = session.get("http://202.118.65.21:8089/menu/s_main.jsp")
#课程查询
##课程查询初始界面
r = session.get("http://202.118.65.21:8089/courseSearchAction.do?temp=1")
soup = BeautifulSoup(r.text)
token = soup.input['value'] #解决token问题
r = session.get("http://202.118.65.21:8089/courseSearchAction.do?org.apache.struts.taglib.html.TOKEN="+token+"&kch=&kcm=&jsm=&xsjc=&skxq=&skjc=&xaqh=&jxlh=&jash=&pageSize=20&showColumn=kkxsjc%23%BF%AA%BF%CE%CF%B5&showColumn=kch%23%BF%CE%B3%CC%BA%C5&showColumn=kcm%23%BF%CE%B3%CC%C3%FB&showColumn=kxh%23%BF%CE%D0%F2%BA%C5&showColumn=xf%23%D1%A7%B7%D6&showColumn=kslxmc%23%BF%BC%CA%D4%C0%E0%D0%CD&showColumn=skjs%23%BD%CC%CA%A6&showColumn=zcsm%23%D6%DC%B4%CE&showColumn=skxq%23%D0%C7%C6%DA&showColumn=skjc%23%BD%DA%B4%CE&showColumn=xqm%23%D0%A3%C7%F8&showColumn=jxlm%23%BD%CC%D1%A7%C2%A5&showColumn=jasm%23%BD%CC%CA%D2&showColumn=bkskrl%23%BF%CE%C8%DD%C1%BF&showColumn=xss%23%D1%A7%C9%FA%CA%FD&pageNumber=0&actionType=1")

soup = BeautifulSoup(r.text)
list = soup.find_all('tr','odd')
for ite in list:
	li = ite.find_all('td')
	l = []
	for t in li:
		l.append(t.text)
	#cursor.execute('insert into courses(department,courseNum,name,courseId,credit,type,teacher,week,day,class,campus,building,classroom,capacity,stuNum) values(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)',l)
conn.commit()
for i in range(2,252):
	r = session.get("http://202.118.65.21:8089/courseSearchAction.do?actionType=2&pageNumber="+str(i))
	soup = BeautifulSoup(r.text)
	list = soup.find_all('tr','odd')
	for ite in list:
		li = ite.find_all('td')
		l = []
		for t in li:
			l.append(t.text)
		cursor.execute('insert into courses(department,courseNum,name,courseId,credit,type,teacher,week,day,class,campus,building,classroom,capacity,stuNum) values(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)',l)
	conn.commit()

```
#### crawlTeachers.py
```python
#! /usr/bin/env python3
# -*- coding:utf-8 -*-

from selenium import webdriver
from bs4 import BeautifulSoup
import time
import requests
import re
import mysql.connector

#登陆mysql
conn = mysql.connector.connect(host='数据库地址',user='用户名', password='密码', database='dlut')
cursor = conn.cursor()

driver = webdriver.Firefox()
driver.get("http://gs1.dlut.edu.cn/newVersion/Front/dsxx/Search.aspx?teacherType=masterTutor")
time.sleep(3)
for i in range(1,249):
    #解析网页
    html = driver.page_source
    soup = BeautifulSoup(html)
    idt = re.compile(r'ctl00_contentBody_TutorSearchResult1_Grid0_DXDataRow\d*')
    list = soup.find_all('tr',id=idt)
    urltemplate = 'http://gs1.dlut.edu.cn/Supervisor/Front/dsxx/new/Default.aspx?WebPageName='
    for ite in list:
        li = ite.find_all('td')
        resList = []
        for i in range(0,9):
            if i<8:
                resList.append(li[i].text)
            elif i==8:
                if  'href' in li[8].a.attrs: #有主页
                    t = li[8].a.attrs['href']
                    t = t.replace('/Supervisor/','')
                    t = t.replace('.page','')
                    resList.append(urltemplate+t)
                    r = requests.get(urltemplate+t)
                    soup = BeautifulSoup(r.text)
                    soup_div = soup.find('div','imgBox')
                    #image
                    t = soup_div.find('img')
                    resList.append(t.attrs['src'])
                    soup_div =  soup.find('div','info')
                    #tel
                    resList.append(soup_div.find(id='lblOfficePhone').text)
                    #email
                    resList.append(soup_div.find(id='lblEmail').text)
                else:
                    resList.append('')
                    resList.append('')
                    resList.append('')
                    resList.append('')
        cursor.execute('insert into teachers(name,gender,department,type1,type2,major1,major2,major3,homepage,image,tel,email) values(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)',resList)
        print(resList[0])
    conn.commit()
    #查找next按钮
    next = driver.find_element_by_xpath("//img[@alt='Next']")
    next.click()
    time.sleep(3)

```
